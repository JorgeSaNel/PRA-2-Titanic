---
title: 'Tipología y ciclo de vida de los datos: PEC 2-Limpieza y análisis de los datos'
author: "Autor: Jorge Santos Neila & Javier Cela López"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {text-align: justify}
</style>

******
# PEC 2 - Limpieza y análisis de los datos
******
## Integrantes
En esta segunda práctica de Tipología y ciclo de via de los datos se realiza un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y uso las herramientas de integración, limpieza, validación
y análisis de las mismas. Esta práctica ha sido realizada en un grupo de dos formado por las siguientes personas:

* [Jorge Santos Neila](https://github.com/JorgeSaNel)
* [Javier Cela López](https://github.com/javcela10)

Asimismo, el código se ha subido al repositorio de [GitHub](https://github.com/JorgeSaNel/PRA-2-Titanic) y es de uso **METER AQUI SI ES CÖDIGO LIBRE Y POR QUÉ**

## Objetivos
Los objetivos que se persiguen mediante el desarrollo de esta actividad práctica son los siguientes:

* Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.
* Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.
* Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.
* Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.
* Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.
* Desarrollar las habilidades de aprendizaje que permita continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.
* Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.

## Competencias
Por último, las competencias que se buscan desarrollar a lo largo de esta práctica del Máster de Data Science de la UOC son:

* Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.
* Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

******
## Organización
A lo largo del código se dará respuesta a las preguntas planteadas por el profesor. Se empezará explicando el dataset seleccionado y los motivos de su elección. A continuación se realizará la tarea de integración y selección de los datos de interés a analizar, seguido del trabajo de limpieza de datos y análisis de datos. El grueso del trabajo estará definido bajo estos dos puntos, ya que son los que más trabajo requieren. Se seguirá con una representación visual de los resultados obtenidos y, para terminar, se expondrán las conclusiones que se han llevado a cabo a lo largo de este ejercicio.

******

# Descripción del dataset
El Conjunto de datos, _Dataset_ en inglés, objeto de análisis se ha obtenido a partir del siguiente enlace en [Kaggle](https://www.kaggle.com/c/titanic/data). No obstante, este es un ejercicio propuesto por Kaggle para hacer predicciones y utilizar algoritmos de machine learning, por lo que el dataset estaba dividido en dos con los siguientes ficheros:

* Train.csv, dataset específico para entrenar el modelo de machine learning.
* Test.csv, dataset específico para testear o probar el modelo realizado.

Debido a que en esta práctica no se van a utilizar modelos de predicción, se han unificado ambos ficheros en uno, llamándolo **titanic-dataset.csv**, el cual está constituido por 12 características (columnas) que presentan 1309 personas (filas o registros). Este dataset esta compuesto por las siguientes variables:

* **PassengerId**: Identificador único del pasajero. Tipo de dato: **integer**.
* **Survived**: Código que identifica si la persona sobrevivió al accidente del Titanic en 1912. Los dos valores que admite esta variable son 0, si la persona no sobrevivió, o 1 en caso contrario. El tipo de dato es un **integer**.
* **Pclass**: Código que identifica la clase en la que viajaba el pasajero. Los valores posibles son;'1', correspondiente a primera clase. '2', clase media. '3', clase baja. El tipo de dato es un **integer**.
* **Name**: Variable descriptiva que indica el nombre de la persona. Esta variable es una cadena de caracteres del tipo **varchar**.
* **Sex**: Código que identifica el sexo de la persona. Los valores posibles son 'male', si la persona era un hombre, o 'female', si la persona era una mujer. El tipo de dato es un **varchar**.
* **Age**: Número que identifica la edad del pasajero. Esta variable es de tipo **numeric**.
* **SibSp**: Número de hermanos o cónyuges que tenía el pasajero a bordo. Esta variable es de tipo **integer**.
* **Parch**: Número de padres o hijos que tenía cada pasajero a bordo. Esta variable es de tipo **integer**.
* **Ticket**: Identificador del ticket del pasajero. Esta variable es de tipo **varchar**.
* **Fare**: Tarifa del pasajero para subir a bordo. Esta variable, aunque el sentido común indica que debería ser un float, R lo lee como un **numeric**
* **Cabin**: Identificador de la cabina del pasajero. Esta variable es de tipo **varchar**.
* **Embarked**: Código de embarque del pasajero. Esta variable tiene los siguientes tres valores posibles; 'C' = Cherbourg, 'Q' = Queenstown, 'S' = Southampton. La variable es de tipo **varchar**.

Para comprobar que las vairables descritas tienen el formato correcto, se procede a leer el fichero y mostrar, con la función 'str', el tipo de dato de cada variable:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos los paquetes R que vamos a usar
library(ggplot2)
library(dplyr)

# Guardamos el conjunto de datos test y train en un único dataset
# titanic <- read.csv('../dataset/titanic-dataset.csv', stringsAsFactors = FALSE)
test <- read.csv('../dataset/test.csv',stringsAsFactors = FALSE)
train <- read.csv('../dataset/train.csv', stringsAsFactors = FALSE)

# Unimos los dos conjuntos de datos en uno solo
titanic_file <- bind_rows(train, test)

# Verificamos la estructura del conjunto de datos
str(titanic_file)

```

## Importancia y objetivos de los análisis
Tras este primer análisis del dataset, la siguiente pregunta que debemos hacernos es por qué es importante este conjunto de datos, y qué problema pretende responder. 

A partir de este conjunto de datos se plantea la problemática de determinar qué variables influyen más sobre la probabilidad de sobrevivir en un accidente como el Titanic, como pueden ser las variables sexo, clase o edad. Además, se podrá proceder a crear modelos de regresión que permitan predecir si un pasajero ha sobrevivido o no en función de sus características y contrastes de hipótesis. Esto ayudará a identificar propiedades interesantes en las muestras que puedan ser inferidas con respecto a la población.

Estos análisis adquieren una gran relevancia ya que nos permite conocer cómo eran las clases sociales de hace 100 años, se puede estudiar su correlación, y evitar así posibles inconsistencias entre clases sociales para los futuros cruceros o eventos, como conseguir que todos tengan la misma probabilidad de sobrevivir independientemente de la situación del pasajero o del dinero que este tenga.

# Integración y selección de los datos
A lo largo de este apartado se dará respuesta a las variables que son interesantes analizar para encontrar la posibilidad de que un pasajero sobreviva al accidente. Podemos dejar fuera todas las variables que, a priori, no tiene relación con la variable objetivo (sobrevivir al accidente), como podrían ser 'Name, 'Cabin' o 'Embarked'. Estas variables, al ser identificadores, no ayudan al modelo. Dicho con otras palabras, no habrá correlación entre estas variables y la posibilidad de sobrevivir. 

Por otro lado, las variables que sí se analizarán son:

* 'Survived' para conocer si el pasajero sobrevivió o no. Esta variable será, en un modelo supervisado, nuestra variable objetivo a predecir.
* A su vez, será interesante conocer si las variables 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch' o 'Fare' tiene correlación con la variable clase. 

Adicionalmente, la variable que queda por comentar es el Identificador único del pasajero, PassengerId'. Esta variable, aunque no va a tener relación con la variable objetivo, está bien dejarla en el análisis ya que es la única que nos permite tener identificado al pasajero.

**NOTA: REVISAR. NO ME GUSTA EL WORDING. El ID tiene sentido dejarlo en un modelo supervisado, pero aquí si no se va a hacer, no sé si tendría sentido**

**NOTA: METER MÄS VARIABLES SI CREEMOS OPORTUNo**

# Limpieza de los datos

Se tratará en este apartado el asunto de la limpieza del conjunto de datos, y nos enfocaremos en dos grandes puntos: la gestión de datos nulos o vacíos (o missing values) y la identificación y tratamiento de valores extremos (o outliers).

## Gestión de datos nulos o vacíos

En primer lugar, identificamos aquellos atributos que tengan valores vacíos. Se seguirá una estrategia individualizada teniendo en cuenta el tipo de atributo del que se trate.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic_file))
colSums(titanic_file=="")

```

Observamos dos tipos de missing values: valores con NA y valores con cadenas vacías. A efectos prácticos, serán tratados de igual forma unos y otros.

Para empezar, como ya comentábamos, la variable Cabin no aporta conocimiento de las personas que sobrevivieron. Es una cadena de caracteres que ni siquiera tiene categorías o valores predeterminados, por lo que no es interesante para el análisis. Eliminamos dicha variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file$Cabin <- NULL

```

La variable de la edad contiene 263 valores vacíos. En principio, imputar los missing values de esta variable usando la media del resto de edades podría ser una buena estrategia. Observamos la distribución de esta variable para determinar si es así. Si la distribución es normal y centrada en la media, utilizaremos este método. Si la distribución estuviese sesgada hacia algún extremo, encontraremos otra manera.

```{r echo=TRUE, message=FALSE, warning=FALSE}

dens <- density(titanic_file[!is.na(titanic_file$Age), "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Como se puede apreciar, la función de densidad está ligeramente escorada (o sesgada) hacia la izquierda. La línea roja representa la mediana, y la verde la media. Este escoramiento hace que la media se desplace hacia valores mayores. Esto puede indicar que existen valores extremos o, simplemente, que existen valores altos que encontramos en menor densidad. REcordemos que la media es una medida sensible a este tipo de valores, y por tanto se desplaza. Aunque no es demasiado acusado, quezá sea mejor utilizar la mediana para imputar los valores faltantes.

Dado que se trata de un porcentaje considerable de missing values (263 / 1309), podemos afinar algo más el análisis para imputar los valores de forma más específica. Dividiremos la muestra en las tres clases distintas en las que puede viajar un pasajero. El razonamiento detrás de esto es que es de esperar que la edad sea mayor en las mejores clases (la gente mayor suele viajar con más comodidades que la gente joven). Por tanto, es fácil imaginarse que el atributo de la clase puede tener que ver con la edad del viajero. Vemos a continuación el detalle.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Clase 1

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 2

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 2)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 3

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 3)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Vemos que, para la clase 1, la distribución se centra más en la media, que es cercana a los 40 años. Esto puede ser debido a que no exista mayor concentración de edades bajas que de edades altas como pasa con la muestra total. Es decir, se cumple lo que pronosticábamos: en mejores clases, la edad suele ser más alta. Confirmamos esto viendo que la media y la mediana coinciden prácticamente. 

Para el caso de la clase 2, también encontramos una distribución más centrada. La media se sitúa algo por debajo de los 30 años, y la mediana con un valor ligeramente menor. De nuevo, esta observación confirma la teoría.

Por último, nos encontramos con una densidad para la clase 3 más escorada, y con una media y medianas por debajo de 25 años.

Como sospechábamos, la edad del viajero sí que parece guardar relación con la clase en la que viaje. Utilizaremos las medianas de cada submuestra para imputar los valores nulos de sus observaciones.

```{r echo=TRUE, message=FALSE, warning=FALSE}
titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"])
titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"])
titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"])
```

Encontramos un valor vacío para la variable Fare y dos valores vacíos para la variable Embarked. Al ser números muy reducidos, podemos utilizar la estrategia de imputarlos con la moda (valor que más aparece) para la variable Embarked y la media para la variable Fare sin preocuparnos de introducir sesgos en el conjunto de datos.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file[is.na(titanic_file$Fare), "Fare"] = mean(titanic_file[!is.na(titanic_file$Fare), "Fare"])

freq <- table(titanic_file[!is.na(titanic_file$Embarked), "Embarked"])
titanic_file[titanic_file$Embarked == "", "Embarked"] = freq[which.max(freq)]

```

Para la variable Survived, al ser muy numerosas las observaciones con valores vacíos (lo cual significa que el riesgo de introducir sesgos al imputar mal los datos es grande), utilizaremos un método algo más sofisticado. Seleccionaremos las 10 observaciones más "próximas" y usaremos el valor de dichas observaciones (moda para variables categóricas, media para variables continuas) para imputar nuestros missing values. Este método se conoce como el de los k vecinos más próximos. Usaremos la función kNN del paquete VIM de R con $k = 10$.

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(VIM)

mv <- titanic_file[, c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]

aux_df <- kNN(mv, variable = c("Survived"), k = 10, dist_var = c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"))

titanic_file$Survived <- aux_df$Survived

```

Para terminar, observamos de nuevo la cantidad de valores vacíos. Si todo ha ido bien, ya no deberíamos ver ninguno.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic_file))
colSums(titanic_file=="")

```

## Identificación y tratamiento de valores extremos

Los valores extremos, outliers o valores fuera de rangos pueden ser identificados de distinta forma en función de la tipología de la variable analizada.

En primer lugar, identificaremos valores fuera de rango en variables categóricas. Se trata de categorías no válidas, distintas maneras de codificar una categoría, etc. Extraeremos los valores únicos de dichas variables para ver si existen valores que haya que corregir.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file %>% group_by(Survived) %>% count()
titanic_file %>% group_by(Pclass) %>% count()
titanic_file %>% group_by(SibSp) %>% count()
titanic_file %>% group_by(Parch) %>% count()
titanic_file %>% group_by(Embarked) %>% count()

```

Como vemos, existen pocas sospechas de que algún valor esté fuera de rango. **(COMO MUCHO VER SI AGRUPAMOS LOS 3, 4, 5, 6 Y 9 DE PARCH, 5 Y 6 DE SIBSP, ETC).**

Para las variables numéricas, usaremos la convención de identificar valores extremos como aquellos que se desvíen más de 1.5 IQR (rango intercuartiles) del primer y tercer cuartil respectivamente. En caso de encontrar algún valor atípico, se aplicará un floor y un cap (valores máximos y mínimos aceptados) que serán, respectivamente, $floor = Q1 - 1.5*IQR$ y $cap = Q3 + 1.5*IQR$. Identificaremos estos outliers de forma visual con un diagrama de tipo boxplot, y también realizaremos el cálculo para mayor seguridad. Otra opción podría ser la de normalizar la variable de manera que se distribuya siguiendo una normal $N(0, 1)$. De esta manera, también se eliminarían los outliers.

```{r echo=TRUE, message=FALSE, warning=FALSE}

boxplot(titanic_file$Age, main = "Boxplot - Age", ylab = "Age")
boxplot(titanic_file$Fare, main = "Boxplot - Fare", ylab = "Age")

```

Como se puede observar, en ambas variables encontramos valores extremos por encima de la media. Vemos a continuación las listas de dichos valores. Además, realizamos la transformación cap y floor correspondiente en cada caso.

```{r echo=TRUE, message=FALSE, warning=FALSE}

IQR_age = quantile(titanic_file$Age)[4] - quantile(titanic_file$Age)[2]
IQR_fare = quantile(titanic_file$Fare)[4] - quantile(titanic_file$Fare)[2]

floor_age = quantile(titanic_file$Age)[2] - (1.5 * IQR_age)
floor_fare = quantile(titanic_file$Fare)[2] - (1.5 * IQR_fare)

cap_age = quantile(titanic_file$Age)[4] + (1.5 * IQR_age)
cap_fare = quantile(titanic_file$Fare)[4] + (1.5 * IQR_fare)

titanic_file[titanic_file$Age < floor_age | titanic_file$Age > cap_age, "Age"]
titanic_file[titanic_file$Fare < floor_fare | titanic_file$Fare > cap_fare, "Fare"]

titanic_file[titanic_file$Age < floor_age, "Age"] = floor_age
titanic_file[titanic_file$Fare < floor_fare, "Fare"] = floor_fare

titanic_file[titanic_file$Age > cap_age, "Age"] = cap_age
titanic_file[titanic_file$Fare > cap_fare, "Fare"] = cap_fare

```

Para comprobar, volvemos a dibujar los boxplot y observamos que ya no existen valores extremos en estas variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}

boxplot(titanic_file$Age, main = "Boxplot - Age", ylab = "Age")
boxplot(titanic_file$Fare, main = "Boxplot - Fare", ylab = "Age")

```

Adicionalmente, se cambia la variable Sexo para que aparezca con un valor de 1 o 2, siendo:

* Si la persona era de sexo masculino, 'male', se le asignará un 1.
* Si la persona era de sexo femenino, 'female', se le asignará un 2.

```{r message= FALSE, warning=FALSE}
titanic_file$Sex <- ifelse(titanic_file$Sex == "male", 1, 2)

# Mostramos los 5 primeros valores para saber que se ha definido bien la nueva variable
head(titanic_file, 5)
```

# Análisis de los datos
## Selección de los grupos de datos de interés
Eliminamos las variables que no son interesantes para analizar si una persona ha sobrevivido. Estas son, el nombre, el ticket, el precio, la cabina, y la puerta de embarque.
```{r echo=TRUE, message=FALSE, warning=FALSE}
titanic_file$Name <- NULL
titanic_file$Ticket <- NULL
titanic_file$Fare <- NULL
titanic_file$Cabin <- NULL
titanic_file$Embarked <- NULL

str(titanic_file)
```   


## Comprobación de la normalidad y homogeneidad de la varianza

### Normalidad
Para la comprobación de que los valores que toman nuestras variables cuantitativas provienen de una población distribuida normalmente, utilizaremos la prueba de normalidad de Anderson-Darling.

Así, se comprueba que para que cada prueba se obtiene un p-valor superior al nivel de significación prefijado $\alpha = 0.05$. Si esto se cumple, entonces se considera que variable en cuestión sigue una distribución normal. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(nortest)
alpha = 0.05
col.names = colnames(titanic_file)

for (i in 1:ncol(titanic_file)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n")
  
  if (is.integer(titanic_file[,i]) | is.numeric(titanic_file[,i])) {
    p_val = ad.test(titanic_file[,i])$p.value
    if (p_val < alpha) {
      cat(col.names[i])
        
      # Format output
      if (i < ncol(titanic_file) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
}
```  
Observamos que ninguna variable sigue una distribución normal.

### Homogeneidad
Seguidamente, pasamos a estudiar la homogeneidad de varianzas mediante la aplicación de un test de Fligner-Killeen. En este caso, estudiaremos esta homogeneidad en cuanto a los grupos conformados por los vehículos que presentan un motor turbo frente a un motor estándar. En el siguiente test, la hipótesis nula consiste en que ambas varianzas son iguales.

```{r echo=TRUE, message=FALSE, warning=FALSE}
fligner.test(Survived ~ Pclass, data = titanic_file)
fligner.test(Survived ~ Sex, data = titanic_file)
fligner.test(Survived ~ Age, data = titanic_file)
fligner.test(Survived ~ SibSp, data = titanic_file)
fligner.test(Survived ~ Parch, data = titanic_file)
```
Puesto que obtenemos un p-valor superior a 0.05 para la variable Edad, 'Age', aceptamos la hipótesis de que las varianzas de ambas muestras son homogéneas. Para el resto de variables, al ser el p-valor inferior al intervalo de confianza, no podemos decir que sean homogéneas.

## Pruebas estadísticas

### Correlaciones
En primer lugar, procedemos a realizar un análisis de correlación entre las distintas variables para determinar cuáles de ellas ejercen una mayor influencia sobre la probabilidad de sobrevivir al accidente.

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggcorrplot)
corr_norm <- cor(titanic_file)

ggcorrplot(corr_norm, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlación con datos Normalizados de Titanic", 
           ggtheme=theme_dark) + theme(plot.title = element_text(hjust = 0.5))
```

Podemos observar que la variable que más correlación tiene con las personas que han sobrevivido el el Sexo, con una correlación de 0.44 sobre 1.

### Hipótesis

¿Tienen más probabilidades de sobrevivir al accidente del titanic los chicos que las chicas?

La segunda prueba estadística que se aplicará consistirá en un contraste de hipótesis sobre dos muestras para determinar si el la probabilidad de sobrevivir es mayor dependiendo del sexo. Para ello, tendremos dos muestras: la primera de ellas se corresponderá con todas las personas y, la segunda, con aquellos que eran hombres.

Se debe destacar que un test paramétrico como el que a continuación se utiliza necesita que los datos sean normales, si la muestra es de tamaño inferior a 30. Como en nuestro caso, n > 30, el contraste de hipótesis siguiente es válido (aunque podría utilizarse un test no paramétrico como el de Mann-Whitney, que podría resultar ser más eficiente para este caso).

Así, se plantea el siguiente contraste de hipótesis de dos muestras sobre la diferencia de medias, el cual es unilateral atendiendo a la formulación de la hipótesis alternativa:

<center>$H_{0}: \mu_{1} = \mu_{2}$</center>
<center>$H_{1}: \mu_{1} > \mu_{2}$</center>

donde μ1 es la media de la población de la que se extrae la primera muestra y μ2 es la media de la población de la que extrae la segunda. 

```{r message= FALSE, warning=FALSE}
titanic_file_hombres <- titanic_file[titanic_file$Sex == 1, ]
```

```{r message= FALSE, warning=FALSE}
t.test(titanic_file, titanic_file_hombres, alternative = "greater")
```

Puesto que obtenemos un p-valor mayor que el valor de significación fijado, aceptamos la hipótesis nula. Por tanto, no podemos concluir que los chicos sobreviviesen más que las chicas, sino que fue por igual.

### Regresión Lineal
Se realiza el modelo de regresión lineal. Se empieza metiendo en el modelo la variable más correlacionada, el sexo, y se irá probando a añadir nuevas variables para ver si el modelo mejora o empeora.

```{r message= FALSE, warning=FALSE}
regresion_sexo <- lm(Survived ~ Sex, data = titanic_file)
summary(regresion_sexo)
```
Se puede observar que el coeficiente de determinación es __Multiple R-squared: 0.193__. Esto quiere decir que un 19.3% del modelo tiene relación entre el la supervivencia y el sexo.

Puesto que este valor es muy bajo, se añade la segunda variable más correlacionada al modelo para ver si mejora.
```{r message= FALSE, warning=FALSE}
regresion_mltp <- lm(Survived ~ Sex + Pclass, data = titanic_file)
summary(regresion_mltp)
```
Observamos que el modelo ha mejorado considerablemente, consiguiendo un **Multiple R-squared:  0.2943**.
```{r message= FALSE, warning=FALSE}
regresion_mltp_3 <- lm(Survived ~ Sex + Pclass + Age, data = titanic_file)
summary(regresion_mltp_3)
```
Añadiendo una nueva variable, el modelo mejora a un 0.32 de $R^{2}$. Podemos comprobar que el modelo no mejora al añadir nuevas variables:
```{r message= FALSE, warning=FALSE}
regresion_mltp_5 <- lm(Survived ~ Sex + Pclass + Age + SibSp + Parch, data = titanic_file)
summary(regresion_mltp_5)
```
Vemos que al añadir estas dos nuevas variables, el modelo se mantiene, por lo que revertimos a la versión anterior y dejamos esta por buena.

### Regresión logarítmia
Adicionalmente, se crea un modelo logarítmico para ver si hay mejora. En este caso, al tener variables dicotómicas, tiene más sentido utilizar un modelo logarítmico.

```{r message= FALSE, warning=FALSE}
regresion_lg <- glm(Survived ~ Sex, data = titanic_file, family = "binomial")
summary(regresion_lg)
```

```{r message= FALSE, warning=FALSE}
regresion_lg <- glm(Survived ~ Sex + Pclass, data = titanic_file, family = "binomial")
summary(regresion_lg)
```


```{r message= FALSE, warning=FALSE}
regresion_lg <- glm(Survived ~ Sex + Pclass + Age, data = titanic_file, family = "binomial")
summary(regresion_lg)
```

Observamos que el modelo es mejor con estas tres variables, puesto que el valor de AIC es el más bajo de las tres regresiones realizadas.

### Predicciones
Ahora podemos predecir si una persona sobrevivió al accidente con los modelos creados, el lineal y el logaritmico:

```{r message= FALSE, warning=FALSE}
newdata <- data.frame(
  Pclass = 1,
  Sex = 2,
  Age = 25
)

# Predecir la supervivencia con el modelo lineal
predict(regresion_mltp_3, newdata)
# Predecir la supervivencia con el modelo logaritmico
predict(regresion_lg, newdata)
```
** POR QUE SALE UN VALOR DE DOS PARA EL MODELO LOGARITMICO???? **

# Representación gráfica de los resultados

** QUÉ GRÁFICAS HAY QUE METER EXACTAMENTE AQUI?? **

# Conclusiones del análisis

******
# Bibliografía
******

**Dataset**

* [1] Selección del dataset utilizado: https://www.kaggle.com/c/titanic/overview

**Libros**

* [2] Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.
* [3] Megan Squire (2015). Clean Data. Packt Publishing Ltd.
* [4] Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.
* [5] Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.
* [6] Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.
* [7] Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.
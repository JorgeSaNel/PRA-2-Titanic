---
title: 'Tipología y ciclo de vida de los datos: PEC 2-Limpieza y análisis de los datos'
author: "Autor: Jorge Santos Neila & Javier Cela López"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {text-align: justify}
</style>

******
# PEC 2 - Limpieza y análisis de los datos
******
## Integrantes
En esta segunda práctica de Tipología y ciclo de via de los datos se realiza un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y uso las herramientas de integración, limpieza, validación
y análisis de las mismas. Esta práctica ha sido realizada en un grupo de dos formado por las siguientes personas:

* [Jorge Santos Neila](https://github.com/JorgeSaNel)
* [Javier Cela López](https://github.com/javcela10)

Asimismo, el código se ha subido al repositorio de [GitHub](https://github.com/JorgeSaNel/PRA-2-Titanic) y es de uso **METER AQUI SI ES CÖDIGO LIBRE Y POR QUÉ**

## Objetivos
Los objetivos que se persiguen mediante el desarrollo de esta actividad práctica son los siguientes:

* Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.
* Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.
* Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.
* Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.
* Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.
* Desarrollar las habilidades de aprendizaje que permita continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.
* Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.

## Competencias
Por último, las competencias que se buscan desarrollar a lo largo de esta práctica del Máster de Data Science de la UOC son:

* Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.
* Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

******
## Organización
A lo largo del código se dará respuesta a las preguntas planteadas por el profesor. Se empezará explicando el dataset seleccionado y los motivos de su elección. A continuación se realizará la tarea de integración y selección de los datos de interés a analizar, seguido del trabajo de limpieza de datos y análisis de datos. El grueso del trabajo estará definido bajo estos dos puntos, ya que son los que más trabajo requieren. Se seguirá con una representación visual de los resultados obtenidos y, para terminar, se expondrán las conclusiones que se han llevado a cabo a lo largo de este ejercicio.

******

# Descripción del dataset
El Conjunto de datos, _Dataset_ en inglés, objeto de análisis se ha obtenido a partir del siguiente enlace en [Kaggle](https://www.kaggle.com/c/titanic/data). No obstante, este es un ejercicio propuesto por Kaggle para hacer predicciones y utilizar algoritmos de machine learning, por lo que el dataset estaba dividido en dos con los siguientes ficheros:

* Train.csv, dataset específico para entrenar el modelo de machine learning.
* Test.csv, dataset específico para testear o probar el modelo realizado.

Debido a que en esta práctica no se van a utilizar modelos de predicción, se han unificado ambos ficheros en uno, llamándolo **titanic-dataset.csv**, el cual está constituido por 12 características (columnas) que presentan 1309 personas (filas o registros). Este dataset esta compuesto por las siguientes variables:

* **PassengerId**: Identificador único del pasajero. Tipo de dato: **integer**.
* **Survived**: Código que identifica si la persona sobrevivió al accidente del Titanic en 1912. Los dos valores que admite esta variable son 0, si la persona no sobrevivió, o 1 en caso contrario. El tipo de dato es un **integer**.
* **Pclass**: Código que identifica la clase en la que viajaba el pasajero. Los valores posibles son;'1', correspondiente a primera clase. '2', clase media. '3', clase baja. El tipo de dato es un **varchar**.
* **Name**: Variable descriptiva que indica el nombre de la persona. Esta variable es una cadena de caracteres del tipo **varchar**.
* **Sex**: Código que identifica el sexo de la persona. Los valores posibles son 'male', si la persona era un hombre, o 'female', si la persona era una mujer. El tipo de dato es un **varchar**.
* **Age**: Número que identifica la edad del pasajero. Esta variable es de tipo **numeric**.
* **SibSp**: Número de hermanos o cónyuges que tenía el pasajero a bordo. Esta variable es de tipo **integer**.
* **Parch**: Número de padres o hijos que tenía cada pasajero a bordo. Esta variable es de tipo **varchar**.
* **Ticket**: Identificador del ticket del pasajero. Esta variable es de tipo **varchar**.
* **Fare**: Tarifa del pasajero para subir a bordo. Esta variable, aunque el sentido común indica que debería ser un float, R lo lee como un **varchar**
* **Cabin**: Identificador de la cabina del pasajero. Esta variable es de tipo **varchar**.
* **Embarked**: Código de embarque del pasajero. Esta variable tiene los siguientes tres valores posibles; 'C' = Cherbourg, 'Q' = Queenstown, 'S' = Southampton. La variable es de tipo **varchar**.

Para comprobar que las vairables descritas tienen el formato correcto, se procede a leer el fichero y mostrar, con la función 'str', el tipo de dato de cada variable:
```{r message= FALSE, warning=FALSE}
# Se lee el el fichero Titanic
Titanic_file <- read.csv("../dataset/titanic-dataset.csv", header=T, sep=",", stringsAsFactors = FALSE)
attach(Titanic_file)

# Se representa el tipo de dato de cada variable
str(Titanic_file)
```

## Importancia y objetivos de los análisis
Tras este primer análisis del dataset, la siguiente pregunta que debemos hacernos es por qué es importante este conjunto de datos, y qué problema pretende responder. 

A partir de este conjunto de datos se plantea la problemática de determinar qué variables influyen más sobre la probabilidad de sobrevivir en un accidente como el Titanic, como pueden ser las variables sexo, clase o edad. Además, se podrá proceder a crear modelos de regresión que permitan predecir si un pasajero ha sobrevivido o no en función de sus características y contrastes de hipótesis. Esto ayudará a identificar propiedades interesantes en las muestras que puedan ser inferidas con respecto a la población.

Estos análisis adquieren una gran relevancia ya que nos permite conocer cómo eran las clases sociales de hace 100 años, se puede estudiar su correlación, y evitar así posibles inconsistencias entre clases sociales para los futuros cruceros o eventos, como conseguir que todos tengan la misma probabilidad de sobrevivir independientemente de la situación del pasajero o del dinero que este tenga.

# Integración y selección de los datos
A lo largo de este apartado se dará respuesta a las variables que son interesantes analizar para encontrar la posibilidad de que un pasajero sobreviva al accidente. Podemos dejar fuera todas las variables que, a priori, no tiene relación con la variable objetivo (sobrevivir al accidente), como podrían ser 'Name, 'Cabin' o 'Embarked'. Estas variables, al ser identificadores, no ayudan al modelo. Dicho con otras palabras, no habrá correlación entre estas variables y la posibilidad de sobrevivir. 

Por otro lado, las variables que sí se analizarán son:

* 'Survived' para conocer si el pasajero sobrevivió o no. Esta variable será, en un modelo supervisado, nuestra variable objetivo a predecir.
* A su vez, será interesante conocer si las variables 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch' o 'Fare' tiene correlación con la variable clase. 

Adicionalmente, la variable que queda por comentar es el Identificador único del pasajero, PassengerId'. Esta variable, aunque no va a tener relación con la variable objetivo, está bien dejarla en el análisis ya que es la única que nos permite tener identificado al pasajero.

**NOTA: REVISAR. NO ME GUSTA EL WORDING. El ID tiene sentido dejarlo en un modelo supervisado, pero aquí si no se va a hacer, no sé si tendría sentido**

**NOTA: METER MÄS VARIABLES SI CREEMOS OPORTUNo**

# Limpieza de los datos
## Gestión de datos nulos o vacíos
## Identificación y tratamiento de valores extremos

# Análisis de los datos
## Selección de los grupos de datos de interés
## Comprobación de la normalidad y homogeneidad de la varianza
## Pruebas estadísticas para comparar los grupos de datos

# Representación gráfica de los resultados

# Conclusiones del análisis

******
# Bibliografía
******

**Dataset**

* [1] Selección del dataset utilizado: https://www.kaggle.com/c/titanic/overview

**Libros**

* [2] Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.
* [3] Megan Squire (2015). Clean Data. Packt Publishing Ltd.
* [4] Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.
* [5] Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.
* [6] Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.
* [7] Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.
---
title: 'Tipología y ciclo de vida de los datos: PEC 2-Limpieza y análisis de los datos'
author: "Autor: Jorge Santos Neila & Javier Cela López"
date: "Mayo 2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: cabecera.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<style>
body {text-align: justify}
</style>

******
# PEC 2 - Limpieza y análisis de los datos
******
## Integrantes
En esta segunda práctica de Tipología y ciclo de via de los datos se realiza un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y uso las herramientas de integración, limpieza, validación
y análisis de las mismas. Esta práctica ha sido realizada en un grupo de dos formado por las siguientes personas:

* [Jorge Santos Neila](https://github.com/JorgeSaNel)
* [Javier Cela López](https://github.com/javcela10)

Asimismo, el código se ha subido al repositorio de [GitHub](https://github.com/JorgeSaNel/PRA-2-Titanic) y Este proyecto está bajo la licencia CC BY-NC-SA 4.0, [Atribución-NoComercial-CompartirIgual 4.0 Internacional](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode), la cual permite:

* Compartir: copiar y redistribuir el material en cualquier medio o formato
* Adaptar: remezclar, transformar y construir a partir del material

Siempre y cuando se sigan los siguientes términos:

* Atribución: Se debe dar crédito de manera adecuada, brindar un enlace a la licencia, e indicar si se han realizado cambios. Puede hacerlo en cualquier forma razonable, pero no de forma tal que sugiera que usted o su uso tienen el apoyo del licenciante.
* Sin uso comercial: No puede hacer uso del material con propósitos comerciales.
* Compartir por igual: Si remezcla, transforma o crea a partir del material, debe distribuir su contribución bajo la misma licencia del original.


## Objetivos
Los objetivos que se persiguen mediante el desarrollo de esta actividad práctica son los siguientes:

* Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.
* Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.
* Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.
* Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.
* Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.
* Desarrollar las habilidades de aprendizaje que permita continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.
* Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.

## Competencias
Por último, las competencias que se buscan desarrollar a lo largo de esta práctica del Máster de Data Science de la UOC son:

* Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.
* Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

******
## Organización
A lo largo del código se dará respuesta a las preguntas planteadas por el profesor. Se empezará explicando el dataset seleccionado y los motivos de su elección. A continuación se realizará la tarea de integración y selección de los datos de interés a analizar, seguido del trabajo de limpieza de datos y análisis de datos. El grueso del trabajo estará definido bajo estos dos puntos, ya que son los que más trabajo requieren. Se seguirá con una representación visual de los resultados obtenidos y, para terminar, se expondrán las conclusiones que se han llevado a cabo a lo largo de este ejercicio.

******

# Descripción del dataset
El Conjunto de datos, _Dataset_ en inglés, objeto de análisis se ha obtenido a partir del siguiente enlace en [Kaggle](https://www.kaggle.com/c/titanic/data). No obstante, este es un ejercicio propuesto por Kaggle para hacer predicciones y utilizar algoritmos de machine learning, por lo que el dataset estaba dividido en dos con los siguientes ficheros:

* Train.csv, dataset específico para entrenar el modelo de machine learning.
* Test.csv, dataset específico para testear o probar el modelo realizado.

Para poder trabajar correctamente con todos los datos, el primer paso que se debe realizar es unificar ambos ficheros en uno sólo. Para ello se utiliza la función _bind-rows_, de la siguiente manera:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Cargamos los paquetes R que vamos a usar
library(ggplot2)
library(dplyr)

# Guardamos el conjunto de datos test y train en un único dataset
test <- read.csv('../dataset/test.csv',stringsAsFactors = FALSE)
train <- read.csv('../dataset/train.csv', stringsAsFactors = FALSE)

# Unimos los dos conjuntos de datos en uno solo
titanic_file <- bind_rows(train, test)

# Verificamos la estructura del conjunto de datos
str(titanic_file)
```

Tras unificar el fichero, podemos observar que está constituido por 12 características (columnas) que presentan 1309 personas (filas o registros). Este dataset esta compuesto por las siguientes variables:

* **PassengerId**: Identificador único del pasajero. Tipo de dato: **integer**.
* **Survived**: Código que identifica si la persona sobrevivió al accidente del Titanic en 1912. Los dos valores que admite esta variable son 0, si la persona no sobrevivió, o 1 en caso contrario. El tipo de dato es un **integer**. Esta es nuestra variable objetivo y la que queremos utilizar para predecir el modelo.
* **Pclass**: Código que identifica la clase en la que viajaba el pasajero. Los valores posibles son;'1', correspondiente a primera clase. '2', clase media. '3', clase baja. El tipo de dato es un **integer**.
* **Name**: Variable descriptiva que indica el nombre de la persona. Esta variable es una cadena de caracteres del tipo **varchar**.
* **Sex**: Código que identifica el sexo de la persona. Los valores posibles son 'male', si la persona era un hombre, o 'female', si la persona era una mujer. El tipo de dato es un **varchar**.
* **Age**: Número que identifica la edad del pasajero. Esta variable es de tipo **numeric**.
* **SibSp**: Número de hermanos o cónyuges que tenía el pasajero a bordo. Esta variable es de tipo **integer**.
* **Parch**: Número de padres o hijos que tenía cada pasajero a bordo. Esta variable es de tipo **integer**.
* **Ticket**: Identificador del ticket del pasajero. Esta variable es de tipo **varchar**.
* **Fare**: Tarifa del pasajero para subir a bordo. Esta variable, aunque el sentido común indica que debería ser un float, R lo lee como un **numeric**
* **Cabin**: Identificador de la cabina del pasajero. Esta variable es de tipo **varchar**.
* **Embarked**: Código de embarque del pasajero. Esta variable tiene los siguientes tres valores posibles; 'C' = Cherbourg, 'Q' = Queenstown, 'S' = Southampton. La variable es de tipo **varchar**.


## Factorización de las variables
A continuación, con el fin de tener el conjunto de datos preparado para la limpieza y análisis del mismo, se analiza qué variables tendría sentido factorizar.

```{r echo=TRUE, message=FALSE, warning=FALSE}
apply(titanic_file,2, function(x) length(unique(x)))
```

La factorización tiene sentido realizarla sobre variables que tienen pocas clases, como son en este caso las variables 'Survived', 'Pclass', 'Sex' o 'Embarked'. Podemos observar que las variables survived y embarked tienen una clase más que las mencionadas en en análisis del dataset. Esto es debido a que, seguramente, tenga algún registro vacío o nulo. Es necesario realizar un pre-análisis de limpieza del conjunto de datos antes de proceder con el modelo. Este apartado se realizará más adelante. Dicho lo cual, se procede a factorizar las variables mencionadas:

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Discretizamos las variables con pocas clases
cols<-c("Survived","Pclass","Sex","Embarked")
for (i in cols){
  titanic_file[,i] <- as.factor(titanic_file[,i])
}

# Después de los cambios, analizamos la nueva estructura del conjunto de datos
str(titanic_file)
```

Se observa que las cuatro variables se han factorizado correctamente.

## Estudio del fichero
A continuación, con el fin de ampliar información sobre el conjunto de datos del fichero, se procede a hacer una representación de los mismos en torno a nuestra variable objetivo, la supervivencia al accidente, variable 'Survived'. No obstante, esta representación se hace sobre fichero _train.csv_, ya que el fichero test no tiene la variable objetivo representada puesto que es la variable que se quiere predecir.

```{r echo=TRUE, message=FALSE, warning=FALSE}
filas=dim(train)[1]
ggplot(data=titanic_file[1:filas,],aes(x=Sex,fill=Survived))+geom_bar() + ggtitle("Representación de la variable Sexo con Supervivencia") + xlab("Sexo") + ylab("Conteo Supervivencia") + theme(plot.title = element_text(hjust = 0.5))
```

Podemos observar que en el crucero hubo casi el doble de chicos que de chicas, aunque sobrevivieron más del doble de mujeres que de hombres.

A continuación se representa el número de hijos de cada familia en relación a la supervivencia:
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Survivial como función de SibSp
ggplot(data = titanic_file[1:filas,],aes(x=SibSp,fill=Survived))+geom_bar() + ggtitle("Representación de la variable Hijos con Supervivencia") + xlab("Nº de Hijos") + ylab("Conteo Supervivencia") + theme(plot.title = element_text(hjust = 0.5))
```

Podemos observar que la gran mayoría de familias que sobrevivieron tenían, de media, menos de dos hijos. Es interesante crear una nueva variable en la que sume el total de miembros en la familia ('SibSp' + 'Parch') para entrenar nuestro modelo. Esto se hará más adelante.

Por último, se muestra la distribucción de la edad de las personas en función de la supervivencia:
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Survival como función de age:
titanic_file1<-titanic_file[1:filas,]

ggplot(data = titanic_file1[!(is.na(titanic_file[1:filas,]$Age)),],aes(x=Age,fill=Survived))+geom_histogram(binwidth =3) + ggtitle("Representación de la Edad con Supervivencia") + xlab("Edad") + ylab("Conteo Supervivencia") + theme(plot.title = element_text(hjust = 0.5))
```

Podemos observar que la gran mayoría de las personas que sobrevivieron al accidente eran menores de 40 años.

## Importancia y objetivos de los análisis
Tras este primer análisis del dataset, la siguiente pregunta que debemos hacernos es por qué es importante este conjunto de datos, y qué problema pretende responder. 

A partir de este conjunto de datos se plantea la problemática de determinar qué variables influyen más sobre la posibilidad de sobrevivir a un accidente como el Titanic, como pueden ser las variables sexo, clase o edad. Además, se podrá proceder a crear modelos de aprendizaje (regresión logarítmica o árbol de decisión) que permitan predecir si un pasajero ha sobrevivido o no en función de sus características y contrastes de hipótesis. Esto ayudará a identificar propiedades interesantes en las muestras que puedan ser inferidas con respecto a la población.

Estos análisis adquieren una gran relevancia ya que nos permite conocer cómo eran las clases sociales de hace 100 años, se puede estudiar su correlación, y evitar así posibles inconsistencias entre clases sociales para los futuros cruceros o eventos, como conseguir que todos tengan la misma probabilidad de sobrevivir independientemente de la situación del pasajero o del dinero que este tenga.

# Integración y selección de los datos
A lo largo de este apartado se dará respuesta a las variables que son interesantes analizar para encontrar la posibilidad de que un pasajero sobreviva al accidente. Podemos dejar fuera todas las variables que, a priori, no tiene relación con la variable objetivo (sobrevivir al accidente), como podrían ser 'PassengerId', 'Name, 'Cabin' o 'Embarked'. Estas variables, al ser identificadores, no ayudan al modelo. Dicho con otras palabras, no habrá correlación entre estas variables y la posibilidad de sobrevivir. 

Por otro lado, las variables que sí se analizarán son:

* 'Survived' para conocer si el pasajero sobrevivió o no. Esta variable será, en un modelo supervisado, nuestra variable objetivo a predecir.
* A su vez, será interesante conocer si las variables 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch' o 'Fare' tiene correlación con la variable clase. 

**NOTA: REVISAR ESTO AL TERMINAR EL TRABAJO POR SI NOS HEMOS DEJADO ALGO**

# Limpieza de los datos

Se tratará en este apartado el asunto de la limpieza del conjunto de datos, y nos enfocaremos en dos grandes puntos: la gestión de datos nulos o vacíos (o missing values) y la identificación y tratamiento de valores extremos (o outliers).

## Gestión de datos nulos o vacíos

En primer lugar, identificamos aquellos atributos que tengan valores vacíos. Se seguirá una estrategia individualizada teniendo en cuenta el tipo de atributo del que se trate.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic_file))
colSums(titanic_file=="")

```

Observamos dos tipos de missing values: valores con NA y valores con cadenas vacías. A efectos prácticos, serán tratados de igual forma unos y otros.

Antes de comenzar, analizaremos también la existencia de ceros en las variables continuas, y nos plantearemos si tiene sentido que existan o si debemos tratarlos como valores vacíos. Estudiaremos las variables Age y Fare.

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(titanic_file[, c("Age", "Fare")] == 0)

```

Encontramos ceros en la variable Fare. A priori, es una variable en la que no tiene sentido la existencia de ceros. Es esperable que todos los pasajeros hayan pagado un precio por su billete, por lo que consideramos que estos datos deberán tratarse como valores vacíos y serán incluidos en el tratamiento.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file[which(titanic_file$Fare == 0), "Fare"] = NA

```

Para empezar, como ya comentábamos, la variable Cabin no aporta conocimiento de las personas que sobrevivieron. Es una cadena de caracteres que ni siquiera tiene categorías o valores predeterminados, por lo que no es interesante para el análisis. Eliminamos dicha variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file$Cabin <- NULL

```

La variable de la edad contiene 263 valores vacíos. En principio, imputar los missing values de esta variable usando la media del resto de edades podría ser una buena estrategia. Observamos la distribución de esta variable para determinar si es así. Si la distribución es normal y centrada en la media, utilizaremos este método. Si la distribución estuviese sesgada hacia algún extremo, encontraremos otra manera.

```{r echo=TRUE, message=FALSE, warning=FALSE}

dens <- density(titanic_file[!is.na(titanic_file$Age), "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Como se puede apreciar, la función de densidad está ligeramente escorada (o sesgada) hacia la izquierda. La línea roja representa la mediana, y la verde la media. Este escoramiento hace que la media se desplace hacia valores mayores. Esto puede indicar que existen valores extremos o, simplemente, que existen valores altos que encontramos en menor densidad. REcordemos que la media es una medida sensible a este tipo de valores, y por tanto se desplaza. Aunque no es demasiado acusado, quezá sea mejor utilizar la mediana para imputar los valores faltantes.

Dado que se trata de un porcentaje considerable de missing values (263 / 1309), podemos afinar algo más el análisis para imputar los valores de forma más específica. Dividiremos la muestra en las tres clases distintas en las que puede viajar un pasajero. El razonamiento detrás de esto es que es de esperar que la edad sea mayor en las mejores clases (la gente mayor suele viajar con más comodidades que la gente joven). Por tanto, es fácil imaginarse que el atributo de la clase puede tener que ver con la edad del viajero. Vemos a continuación el detalle.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Clase 1

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 1)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 2

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 2)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

# Clase 3

dens <- density(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Age (clase 3)")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Vemos que, para la clase 1, la distribución se centra más en la media, que es cercana a los 40 años. Esto puede ser debido a que no exista mayor concentración de edades bajas que de edades altas como pasa con la muestra total. Es decir, se cumple lo que pronosticábamos: en mejores clases, la edad suele ser más alta. Confirmamos esto viendo que la media y la mediana coinciden prácticamente. 

Para el caso de la clase 2, también encontramos una distribución más centrada. La media se sitúa algo por debajo de los 30 años, y la mediana con un valor ligeramente menor. De nuevo, esta observación confirma la teoría.

Por último, nos encontramos con una densidad para la clase 3 más escorada, y con una media y medianas por debajo de 25 años.

Como sospechábamos, la edad del viajero sí que parece guardar relación con la clase en la que viaje. Utilizaremos las medianas de cada submuestra para imputar los valores nulos de sus observaciones.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 1, "Age"])
titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 2, "Age"])
titanic_file[is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"] = median(titanic_file[!is.na(titanic_file$Age) & titanic_file$Pclass == 3, "Age"])

```

Para la variable Fare, hacemos el mismo análisis para ver su distribución y determinar si la media o la mediana pueden ser buenos valores para sustituir el valor faltante.

```{r echo=TRUE, message=FALSE, warning=FALSE}

dens <- density(titanic_file[!is.na(titanic_file$Fare), "Fare"])

n <- length(dens$y)                       #$
dx <- mean(diff(dens$x))                  # Typical spacing in x $
y.unit <- sum(dens$y) * dx                # Check: this should integrate to 1 $
dx <- dx / y.unit                         # Make a minor adjustment
x.mean <- sum(dens$y * dens$x) * dx
y.mean <- dens$y[length(dens$x[dens$x < x.mean])] #$
x.mode <- dens$x[i.mode <- which.max(dens$y)]
y.mode <- dens$y[i.mode]                  #$
y.cs <- cumsum(dens$y)                    #$
x.med <- dens$x[i.med <- length(y.cs[2*y.cs <= y.cs[n]])] #$
y.med <- dens$y[i.med] 

plot(dens, col = "blue", main="Densidad - Fare")
mapply(function(x,y,c) lines(c(x,x), c(0,y), lwd=2, col=c), c(x.mean, x.med), c(y.mean, y.med), c("green", "red"))

```

Como se puede observar, la variable Fare está lejos de distribuirse de forma normal. Usar medidas estadísticas como la media o la mediana no sería la estrategia más acertada. Es de esperar que el valor de esta variable esté relacionado con otras de las del conjunto de datos, como puede ser la clase en la que se viaja, el puerto en el que se embarca, la edad, etc. Es por eso que lo más adecuado para imputar esta variable sería utilizar el algoritmo de vecinos próximos. Seleccionaremos las 10 observaciones más "próximas" en función de la distancia entre las variables observadas y usaremos el valor de dichas observaciones (media para variables continuas) para imputar nuestros missing values. Usaremos la función kNN del paquete VIM de R con $k = 10$.

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(VIM)

mv <- titanic_file[, c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]

aux_df <- kNN(mv, variable = c("Fare"), k = 10, dist_var = c("Pclass", "Sex", "Age", "SibSp", "Parch", "Survived", "Embarked"))

titanic_file$Fare <- aux_df$Fare

```

Encontramos dos valores vacíos para la variable Embarked. Al ser números muy reducidos y tratarse de una variable categórica, podemos utilizar la estrategia de imputarlos con la moda (valor que más aparece) sin preocuparnos de introducir sesgos en el conjunto de datos.

```{r echo=TRUE, message=FALSE, warning=FALSE}

uv <- unique(titanic_file$Embarked)
titanic_file[is.na(titanic_file$Embarked) | titanic_file$Embarked == "", "Embarked"] = uv[which.max(tabulate(match(titanic_file$Embarked, uv)))]

```

En el caso de la variable Survived, quizá no tenga sentido imputar estas observaciones vacías, ya que se trata de la variable objetivo. Son precisamente estos valores faltantes los que trataremos de predecir cuando construyamos un modelo sobre este conjunto de datos.

Para terminar, observamos de nuevo la cantidad de valores vacíos. Si todo ha ido bien, ya no deberíamos ver ninguno (a excepción de la variable Survived, naturalmente).

```{r echo=TRUE, message=FALSE, warning=FALSE}

colSums(is.na(titanic_file))
colSums(titanic_file=="")

```

## Identificación y tratamiento de valores extremos

Los valores extremos, outliers o valores fuera de rangos pueden ser identificados de distinta forma en función de la tipología de la variable analizada.

En primer lugar, identificaremos valores fuera de rango en variables categóricas. Se trata de categorías no válidas, distintas maneras de codificar una categoría, etc. Extraeremos los valores únicos de dichas variables para ver si existen valores que haya que corregir.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file %>% group_by(Survived) %>% count()
titanic_file %>% group_by(Pclass) %>% count()
titanic_file %>% group_by(SibSp) %>% count()
titanic_file %>% group_by(Parch) %>% count()
titanic_file %>% group_by(Embarked) %>% count()

```

Como vemos, existen pocas sospechas de que algún valor esté fuera de rango. **(COMO MUCHO VER SI AGRUPAMOS LOS 3, 4, 5, 6 Y 9 DE PARCH, 5 Y 6 DE SIBSP, ETC).**

Para las variables numéricas, usaremos la convención de identificar valores extremos como aquellos que se desvíen más de 1.5 IQR (rango intercuartiles) del primer y tercer cuartil respectivamente. En caso de encontrar algún valor atípico, se aplicará un floor y un cap (valores máximos y mínimos aceptados) que serán, respectivamente, $floor = Q1 - 1.5*IQR$ y $cap = Q3 + 1.5*IQR$. Identificaremos estos outliers de forma visual con un diagrama de tipo boxplot, y también realizaremos el cálculo para mayor seguridad. Otra opción podría ser la de normalizar la variable de manera que se distribuya siguiendo una normal $N(0, 1)$. De esta manera, también se eliminarían los outliers.

Sin embargo, discutiremos si los outliers encontrados son valores realmente erróneos o si por el contrario se encuentran en el rango esperable para cada variable. Por ejemplo, sería evidente que una edad de 150 años es un outlier y habría que transformarlo. Pero quizá una edad de 90 no lo sea, y aún así esté siendo identificado como outlier por nuestro método al existir pocas observaciones de personas con 90 años.

```{r echo=TRUE, message=FALSE, warning=FALSE}

boxplot(titanic_file$Age, main = "Boxplot - Age", ylab = "Age")
boxplot(titanic_file$Fare, main = "Boxplot - Fare", ylab = "Age")

```

Como se puede observar, en ambas variables encontramos valores extremos por encima de la media. Vemos a continuación las listas de dichos valores. Además, si fuera procedente, realizaríamos la transformación cap y floor correspondiente en cada caso.

```{r echo=TRUE, message=FALSE, warning=FALSE}

IQR_age = quantile(titanic_file$Age)[4] - quantile(titanic_file$Age)[2]
IQR_fare = quantile(titanic_file$Fare)[4] - quantile(titanic_file$Fare)[2]

floor_age = quantile(titanic_file$Age)[2] - (1.5 * IQR_age)
floor_fare = quantile(titanic_file$Fare)[2] - (1.5 * IQR_fare)

cap_age = quantile(titanic_file$Age)[4] + (1.5 * IQR_age)
cap_fare = quantile(titanic_file$Fare)[4] + (1.5 * IQR_fare)

sort(titanic_file[titanic_file$Fare < floor_fare | titanic_file$Fare > cap_fare, "Fare"])
sort(titanic_file[titanic_file$Age < floor_age | titanic_file$Age > cap_age, "Age"])

```

En las listas de datos proporcionadas (ordenadas de mayor a menor), se puede apreciar que el valor máximo para la edad es de 80 años, lo cual es más que posible que se trate de un valor correcto, y que por tanto no debemos transformar. Siguiendo el mismo razonamiento, el resto de valores también permanecerán intactos.

Por parte de la variable Fare, encontramos varios valores inusualmente altos. Se trata de un precio de billete por encima de los 500 dólares. Es sospechoso, pero encontramos 4 observaciones distintas con exactamente el mismo precio. Esto nos sugiere que perfectamente se puede tratar de un billete de lujo. De ser así, debería tratarse de un viajero de primera clase. Es una comprobación muy superficial, pero nos indicará si nuestra hipótesis va por buen camino o no.

```{r echo=TRUE, message=FALSE, warning=FALSE}

titanic_file[titanic_file$Fare == max(titanic_file$Fare), c("Pclass", "Fare")]

```

Como sospechábamos, se trata de viajeros de primera clase. Esto sumado a que los cuatro contienen el mismo valor en la variable Fare nos lleva a pensar que es un valor correcto, por lo que no deberemos transformarlo tampoco.

# Análisis de los datos
A lo largo de este apartado se hará un análisis del conjunto de datos con el fin de obtener un modelo para predecir si una persona puede sobrevivir o no a un accidente como el Titanic dadas unas variables. En primer lugar, se seleccionarán los grupos de interés, seguido de un análisis de normalidad y homogeneidad. Por último, se harán unas pruebas estadísticas para obtener el modelo que mejor se adapte al conjunto de datos.

No obstante, puesto que el análisis se va a realizar con el conjunto de datos cuya variable Survived está informada para obtener mejores resultados en el modelo, se procede a modificar el set:
```{r echo=TRUE, message=FALSE, warning=FALSE}
titanic_file_train <- titanic_file[!is.na(titanic_file$Survived),]
```

## Selección de los grupos de datos de interés
En primer lugar, se seleccionan las variables que son interesantes dejar en el modelo para conocer si una persona ha sobrevivido al accidente. Para empezar, podemos eliminar aquellas que, claramente, no son relevantes para el modelo, como son las variables 'PassengerID', 'Name', 'Ticket' o 'Cabin'. Estas variables, por su definición, no representan nada para el modelo. Es decir, aunque entrenemos al modelo con estas variables, no va a ver una mejoría, sino que seguramente empeore el modelo haciéndolo más complejo.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Se eliminan las variables
titanic_file_train$PassengerId <- NULL
titanic_file_train$Name <- NULL
titanic_file_train$Ticket <- NULL
titanic_file_train$Cabin <- NULL

# Se comprueba que no aparecen en el fichero
str(titanic_file_train)
```   

No obstante, es necesario realizar un análisis con el resto de variables para conocer si merece la pena eliminarlas o no. Sin embargo, antes de hacer este análisis, vemos conveniente crear una nueva variable que represente el tamaño de la familia haciendo la suma entre las variables 'SibSp', número de hermanos, y 'Parch', número de padres o hijos. Esta nueva variable se llamará **FamilySize**.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Construimos un atributo nuevo: family size.
titanic_file_train$FamilySize <- titanic_file_train$SibSp + titanic_file_train$Parch +1

# Observamos su distribucción
titanic_file1 <- titanic_file_train[1:filas,]
ggplot(data = titanic_file1[!is.na(titanic_file_train[1:filas,]$FamilySize),],aes(x=FamilySize,fill=Survived))+geom_histogram(binwidth =1,position="fill")+ylab("Frecuencia") + ggtitle("Representación de la nueva variable Tamaño de la familia con la variable objetivo") + xlab("Tamaño de la familia") + ylab("Conteo Supervivencia") + theme(plot.title = element_text(hjust = 0.5))
```

Podemos sacar como conclusión que aquellas familias compuestas por menos integrantes sobrevivieron más, en porcentaje, que aquellas con más de 5 miembros.

Tras la construcción de esta nueva variable, procedemos a realizar el análisis comentado para ver qué variables son representativas en el modelo. Este análisis es diferente en función del tipo de la variable, utilizando:

* **Correlación** para aquellas variables que numéricas, como 'Age', 'Fare', 'SibSp', 'Parch', o la nueva variable 'FamilySize'. Estas variables, aunque sean de tipo integer, se corresponden con variables numéricas por lo que será necesario convertirlas al tipo de dato correcto.
* **Test Chi Cuadrado** para aquellas variables que son categóricas, 'Pclass', 'Sex' o 'Embarked' 

### Correlación
Así pues, se procede a realizar un análisis de correlación entre las distintas variables para determinar cuáles de ellas ejercen una mayor influencia sobre la variable objetivo.

Tal y como se ha comentado, el primer paso es modificar las variables del número de hermanos y de hijos/padres para que sea del tipo numérico.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Se convierten las variables al tipo numérico
titanic_file_train$SibSp <- as.numeric(titanic_file_train$SibSp)
titanic_file_train$Parch <- as.numeric(titanic_file_train$Parch)

# Se comprueba que el tipo de dato es correcto.
str(titanic_file_train)
```

El siguiente paso será analizar dichas variables con una correlación. Para ello se obtiene solamente las variables mencionadas:
```{r echo=TRUE, message=FALSE, warning=FALSE}
titanic_corr <- titanic_file_train %>% select('Age', 'Fare', 'SibSp', 'Parch', 'FamilySize')

# Pequeña comprobación de que la copia se ha hecho correctamente
head(titanic_corr, 2)
```

Dicho lo cual, se procede a analizar cuál es la correlación entre dichas variables.
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ggcorrplot)
corr_norm <- cor(titanic_corr)

ggcorrplot(corr_norm, hc.order = TRUE, 
           type = "lower", 
           lab = TRUE, 
           lab_size = 3, 
           method="circle", 
           colors = c("tomato2", "white", "springgreen3"), 
           title="Correlación con datos Normalizados de Titanic", 
           ggtheme=theme_dark) + theme(plot.title = element_text(hjust = 0.5))
```

Podemos observar que la correlación entre las variables suele ser bastante baja, a excepción de la nueva variable creada, FamilySize, que tiene una alta correlación con las variables 'SibSp' y 'Parch'. Así pues, con el fin de optimizar el modelo y no añadir variables con alta correlación, se eliminan las dos variables utilizadas para crear el Tamaño de la familia del fichero original.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Se eliminan las variables
titanic_file_train$SibSp <- NULL
titanic_file_train$Parch <- NULL

# Se comprueba que no aparecen en el fichero
str(titanic_file_train)
``` 

### Test Chi Cuadrado
Por último, se analiza si existe relación entre las variables categóricas con la variable objetivo, que son las variables 'Pclass', 'Sex' o 'Embarked'. Para ello, se utiliza el test Chi-cuadrado de Pearson. Un resultado significativo nos dirá que existe asociación entre ambas variables. Este apartado se realiza bajo la función _chisq.test_, la cual recibe por parámetro la tabla de contingencia entre la variable objetivo y la variable a estudiar.

Respecto al análisis de los resultados, se confirmará que las variables son dependientes y tienen relación entre sí siempre y cuando el valor del test sea inferior al valor significativo, el cual es de 0.05, y por tanto se rechaza la hipótesis nula de independencia. Esta hipótesis está definida de la siguiente manera:

<center>$H_{0}$: La variable objetivo y la variable a analizar ('Pclass', 'Sex' o 'Embarked') son independientes</center>
<center>$H_{1}$: La variable objetivo y la variable a analizar ('Pclass', 'Sex' o 'Embarked') no son independientes</center>

#### Variable objetivo con Pclass
Dicho lo cual, se genera la tabla de contingencia entre la variable objetivo y la primera variable a analizar, las clases sociales.
```{r message= FALSE, warning=FALSE}
#install.packages("knitr") # Tabla [14]
library(knitr)
#install.packages("kableExtra") # Tabla [14]
library(kableExtra)

# Creamos la tabla de contingencia
tc_pclass <- table(titanic_file_train$Survived, titanic_file_train$Pclass)
# Le damos formato a la tabla de contingencia
tc_pclass %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

A simple vista, se puede confirmar que la gran mayoría de muertos al accidente del Titanic eran aquellos que pertenecían a la tercera clase. A continuación se realiza el Test con la tabla de contingencia:
```{r message= FALSE, warning=FALSE}
chisq.test(tc_pclass) 
```
Puesto que el p-valor es muy cercano a 0, se debe rechazar la hipótesis nuela y afirmar que existe relación entre las variables.

#### Variable objetivo con Sexo
De la misma manera, se realiza el mismo análisis entre la variable objetivo y la variable sexo:

```{r message= FALSE, warning=FALSE}
# Creamos la tabla de contingencia
tc_sex <- table(titanic_file_train$Survived, titanic_file_train$Sex)
# Le damos formato a la tabla de contingencia
tc_sex %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

A simple vista, se puede confirmar que la gran mayoría de los muertos eran hombres, casi 4 veces más que las mujeres. A continuación se realiza el Test con la tabla de contingencia:
```{r message= FALSE, warning=FALSE}
chisq.test(tc_sex) 
```
Tal y como se ha dicho anteriormente, debido a que el valor de p es inferior al valor significativo de 0.05, debemos rechazar la hipótesis nula y afirmar que las variables son dependientes entre sí.

#### Variable objetivo con Embarked
Por último, toca analizar la relación entre la variable objetivo y la variable de embarcación. Primero, se realiza la tabla de contingencia:
```{r message= FALSE, warning=FALSE}
# Creamos la tabla de contingencia
tc_embarked <- table(titanic_file_train$Survived, titanic_file_train$Embarked)
tc_embarked <- tc_embarked[,-1]

# Le damos formato a la tabla de contingencia
tc_embarked %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = F)
```


Una vez obtenida la tabla de contingencia, se ejecuta la función para conseguir el valor de Chi-Square, obteniendo un valor inferior al valor significativo de 0.05. Así pues, se debe rechazar la hipótesis nula y afirmar que las variables tienen relación entre sí.
```{r message= FALSE, warning=FALSE}
chisq.test(tc_embarked) 
```

#### Conclusiones del análisis de las variables
Tras el análisis de las variables realizado, llegamos a la conclusión de que las siguientes variables son interesantes utilizarlas para crear el modelo en base a nuestra variable objetivo, la supervivencia al accidente:
```{r message= FALSE, warning=FALSE}
str(titanic_file_train)
```

## Comprobación de la normalidad y homogeneidad de la varianza
A lo largo de este apartado se estudiará la normalidad y homogeneidad de las variables.

### Normalidad
La comprobación de normalidad se realiza para aquellas variables que son numéricas, que en este caso son 'Age', 'Fare' y la nueva variable creada, 'FamilySize'.

Para la comprobación de que los valores que toman nuestras variables cuantitativas provienen de una población distribuida normalmente, utilizaremos la prueba de normalidad de Anderson-Darling. Así, se comprueba que para que cada prueba se obtiene un p-valor superior al nivel de significación prefijado $\alpha = 0.05$. Si esto se cumple, entonces se considera que variable en cuestión sigue una distribución normal. 
```{r echo=TRUE, message=FALSE, warning=FALSE}
library(nortest)
alpha = 0.05
col.names = colnames(titanic_file_train)

for (i in 1:ncol(titanic_file_train)) {
  if (i == 1) cat("Variables que no siguen una distribución normal:\n")
  
  if (is.integer(titanic_file_train[,i]) | is.numeric(titanic_file_train[,i])) {
    p_val = ad.test(titanic_file_train[,i])$p.value
    if (p_val < alpha) {
      cat(col.names[i])
        
      # Format output
      if (i < ncol(titanic_file_train) - 1) cat(", ")
      if (i %% 3 == 0) cat("\n")
    }
  }
}
```  

Observamos que ninguna de las tres variables numéricas siguen una distribución normal.

### Homogeneidad
Seguidamente, pasamos a estudiar la homogeneidad de varianzas mediante la aplicación de un test de Fligner-Killeen. Este test se aplica para las variables que no siguen una distritucción de normalidad, como son las tres variables numéricas que se ha analizado anteriormente. En el siguiente test, la hipótesis nula consiste en que ambas varianzas son diferentes, es decir:

<center>$H_{0}$: La variable objetivo y la variable a analizar tienen diferente varianza</center>
<center>$H_{1}$: La variable objetivo y la variable a analizar tienen la misma varianza</center>

Como siempre, se aceptará la hipótesis nula siempre y cuando el P-Valor sea inferior al valor significativo de 0.05. 

Se ejecuta el test de Fligner-Killeen para todas las variables:
```{r echo=TRUE, message=FALSE, warning=FALSE}
fligner.test(as.numeric(Survived) ~ Age, data = titanic_file_train)
fligner.test(as.numeric(Survived) ~ Fare, data = titanic_file_train)
fligner.test(as.numeric(Survived) ~ FamilySize, data = titanic_file_train)
```

Se puede observar que para las variables de Edad y Fare se obtiene un p-valor superior a 0.05 (Age ~ 0.9 y Fare ~ 0.32) por lo que se debe aceptar  la hipótesis de que las varianzas de ambas muestras son homogéneas. Para el resto de variables, al ser el p-valor inferior al intervalo de confianza, no podemos decir que sean homogéneas.

## Modelos de predicción
En el último apartado de este punto de análisis estadístico, se procede a realizar tres modelos de predicción diferentes y comparar cuál es el mejor.

* En primer lugar, se utilizará una regresión múltiple logarítmica ya que la variable objetivo es dicotómica. En caso de que no fuese una variable dicotómica, tendría más sentido realizar una regresión múltiple lineal.
* A continuación se utilizará el modelo de clasificación supervisado C50 que corresponde a un árbol de decisión.
* Por último, se utilizará otro algoritmo de árbol de decisión como es el RPART.

### Regresión logarítmia
En primer lugar, se procede a generar una regresión logarítmica múltiple con la función __glm__. Esta función recibe tres parámetros, siendo la primera la variable objetivo, la segunda las variables independientes, y en tercer lugar el set de datos. Adicionalmente, le indicamos que la familia elegida para el modelo es la binomial puesto que la variable objetivo es dicotómica.

Tal y como se ha visto, las 6 variables analizadas, a parte de la variable objetivo, son relevantes para el modelo. No obstante, se crean diferentes modelos logarítmicos con diferentes variables para analizar cúal es el que mejor resultados obtiene.
```{r message= FALSE, warning=FALSE}
# Se crean 5 modelos con diferentes variables.
modelo_glm_1 <- glm(Survived ~ Pclass + Sex + Embarked, data = titanic_file_train, family = "binomial")
modelo_glm_2 <- glm(Survived ~ Age + Fare + FamilySize, data = titanic_file_train, family = "binomial")
modelo_glm_3 <- glm(Survived ~ Pclass + Sex + Age + Fare, data = titanic_file_train, family = "binomial")
modelo_glm_4 <- glm(Survived ~ Sex + Fare + Embarked + FamilySize, data = titanic_file_train, family = "binomial")
modelo_glm_5 <- glm(Survived ~ Pclass + Sex + Age + Fare + Embarked + FamilySize, data = titanic_file_train, family = "binomial")
```

Una vez se tienen los modelos, se comprueba cuál es el mejor. Para ello se obtiene el R^2 de cada modelo y se compara con los demás. Para obtener este valor se utiliza la función _rsq_ de la librería RSQ. La teória indica que el mejor modelo será aquel que tenga el valor R^2 más alto.
```{r message= FALSE, warning=FALSE}
library(rsq)
# Se crea la tabla de coeficientes
tabla.coeficientes <- matrix(c(1, rsq(modelo_glm_1, adj=TRUE), 
                               2, rsq(modelo_glm_2, adj=TRUE), 
                               3, rsq(modelo_glm_3, adj=TRUE), 
                               4, rsq(modelo_glm_4, adj=TRUE), 
                               5, rsq(modelo_glm_5, adj=TRUE)), 
                             ncol = 2, byrow = TRUE
                      )

# Se muestra por pantalla la matriz creada
colnames(tabla.coeficientes) <- c("Modelo", "R^2")
tabla.coeficientes
```
Como podemos observar, el modelo que mejor resultado da es el último, el modelo 5, el cual contiene las 6 variables. No obstante, cabe mencionar que el modelo 3, el cual se ha realizado en base a las variables categóricas más la variable numérica Fare, que son dos variables menos, tiene unos resultados bastantes parecidos al modelo 5. 

Otra manera de obtener el mejor modelo es con el parámetro AIC, Akaike information criterion. Cuanto más bajo sea el parámetro, mejor será el modelo. Se comprueba este parámetro para los dos mejores modelos, el modelo 3 y el 5.
```{r message= FALSE, warning=FALSE}
summary(modelo_glm_3$aic)
summary(modelo_glm_5$aic)
```

Se puede comprobar que el parámetro AIC para el Modelo 3 es de 814, mientras que en el Modelo 5 tiene un valor de 801. Así pues, entre los modelos de regresión logarítmico, el mejor modelo es el utilizado con todas las variables, el **Modelo 5**.

### Árbol de decisión - C50
```{r message= FALSE, warning=FALSE}
set.seed(1)
titanic_file_random <- titanic_file_train[sample(nrow(titanic_file_train)),]

set.seed(666)
# Variable objetivo en Y
y <- titanic_file_random[,1] 
# Resto de variables en X
X <- titanic_file_random[,2:7] 
```



### Predicciones

#### Modelo Logarítmico
```{r message= FALSE, warning=FALSE}
new_person <- data.frame(
  Pclass = "1",
  Sex = "male",
  Age = 25,
  Fare = 80,
  Embarked = "C",
  FamilySize = 1
)

# Predecir la supervivencia con el modelo logaritmico
predict(modelo_glm_5, new_person)

```

# Representación gráfica de los resultados

** QUÉ GRÁFICAS HAY QUE METER EXACTAMENTE AQUI?? **

# Conclusiones del análisis

******
# Bibliografía
******

**Dataset**

* [1] Selección del dataset utilizado: https://www.kaggle.com/c/titanic/overview

**Libros**

* [2] Calvo M., Subirats L., Pérez D. (2019). Introducción a la limpieza y análisis de los datos. Editorial UOC.
* [3] Megan Squire (2015). Clean Data. Packt Publishing Ltd.
* [4] Jiawei Han, Micheine Kamber, Jian Pei (2012). Data mining: concepts and techniques. Morgan Kaufmann.
* [5] Jason W. Osborne (2010). Data Cleaning Basics: Best Practices in Dealing with Extreme Scores. Newborn and Infant Nursing Reviews; 10 (1): pp. 1527-3369.
* [6] Peter Dalgaard (2008). Introductory statistics with R. Springer Science & Business Media.
* [7] Wes McKinney (2012). Python for Data Analysis. O’Reilley Media, Inc.

** URLs **
* Análisis del R^2 dado un modelo de regresión logarítmico.